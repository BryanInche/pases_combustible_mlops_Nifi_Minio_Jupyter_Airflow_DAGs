{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18ec6a6-1140-40ad-a4d7-0d2ec92b5ed3",
   "metadata": {},
   "source": [
    "#### Leemos el archivo parquet, que se subio al MINIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44d530a-1f13-4e07-b92d-759462f54825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_ciclo_acarreo</th>\n",
       "      <th>id_cargadescarga</th>\n",
       "      <th>id_palas</th>\n",
       "      <th>id_equipo_camion</th>\n",
       "      <th>id_ciclo_carguio</th>\n",
       "      <th>id_equipo_carguio</th>\n",
       "      <th>id_trabajador_pala</th>\n",
       "      <th>id_guardia_realiza_carga_al_camion</th>\n",
       "      <th>id_locacion</th>\n",
       "      <th>id_poligono_se_obtiene_material</th>\n",
       "      <th>...</th>\n",
       "      <th>id_cargadescarga_pases</th>\n",
       "      <th>coord_x_pases</th>\n",
       "      <th>coord_y_pases</th>\n",
       "      <th>coord_z_pases</th>\n",
       "      <th>angulo_giro_pases</th>\n",
       "      <th>tonelaje_pases</th>\n",
       "      <th>duracion_excavacion_pases</th>\n",
       "      <th>angulo_giro_promedio_pases</th>\n",
       "      <th>has_block_pases</th>\n",
       "      <th>rownum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2606466</td>\n",
       "      <td>2606466</td>\n",
       "      <td>2964682</td>\n",
       "      <td>41</td>\n",
       "      <td>2964682</td>\n",
       "      <td>26</td>\n",
       "      <td>221</td>\n",
       "      <td>2</td>\n",
       "      <td>44919</td>\n",
       "      <td>59473</td>\n",
       "      <td>...</td>\n",
       "      <td>2606466.0</td>\n",
       "      <td>[20463338, 20463338, 20463334, 20463338, 20463...</td>\n",
       "      <td>[839692963, 839692966, 839692968, 839692971, 8...</td>\n",
       "      <td>[418527, 418521, 418523, 418516, 418517, 418524]</td>\n",
       "      <td>[87, 81, 79, 71, 65, 92, 91, 85, 75, 65, 69, 9...</td>\n",
       "      <td>[760, 393, 356, 453, 343]</td>\n",
       "      <td>[26, 42, 22, 30, 32, 32700]</td>\n",
       "      <td>80.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2606467</td>\n",
       "      <td>2606467</td>\n",
       "      <td>2964686</td>\n",
       "      <td>43</td>\n",
       "      <td>2964686</td>\n",
       "      <td>27</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>44941</td>\n",
       "      <td>59547</td>\n",
       "      <td>...</td>\n",
       "      <td>2606467.0</td>\n",
       "      <td>[20444297, 20444553, 20444495, 20444418, 20444...</td>\n",
       "      <td>[839667711, 839667632, 839667540, 839667665, 8...</td>\n",
       "      <td>[418456, 418454, 418455, 418455, 418457, 41846...</td>\n",
       "      <td>[39, 1, 0, 1, 1, 21, 16, 1, 4, 3, 19, 13, 25, ...</td>\n",
       "      <td>[768, 340, 337, 340, 297, 295]</td>\n",
       "      <td>[30, 36, 38, 38, 10, 28, 10, 32700]</td>\n",
       "      <td>14.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2606468</td>\n",
       "      <td>2606468</td>\n",
       "      <td>2964690</td>\n",
       "      <td>44</td>\n",
       "      <td>2964690</td>\n",
       "      <td>27</td>\n",
       "      <td>232</td>\n",
       "      <td>2</td>\n",
       "      <td>44941</td>\n",
       "      <td>59547</td>\n",
       "      <td>...</td>\n",
       "      <td>2606468.0</td>\n",
       "      <td>[20444531, 20444317, 20444366, 20444322, 20445...</td>\n",
       "      <td>[839668575, 839668163, 839668082, 839668073, 8...</td>\n",
       "      <td>[418459, 418455, 418457, 418457, 418456, 41846...</td>\n",
       "      <td>[46, 29, 20, 22, 33, 35, 40, 46, 55, 17, 21, 1...</td>\n",
       "      <td>[643, 280, 324, 238, 297, 344, 229]</td>\n",
       "      <td>[40, 34, 38, 26, 12, 42, 38, 32700]</td>\n",
       "      <td>34.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2606469</td>\n",
       "      <td>2606469</td>\n",
       "      <td>2964684</td>\n",
       "      <td>52</td>\n",
       "      <td>2964684</td>\n",
       "      <td>24</td>\n",
       "      <td>246</td>\n",
       "      <td>2</td>\n",
       "      <td>44939</td>\n",
       "      <td>59557</td>\n",
       "      <td>...</td>\n",
       "      <td>2606469.0</td>\n",
       "      <td>[20074133, 20074149, 20074146, 20074163, 20074...</td>\n",
       "      <td>[839976228, 839976242, 839976263, 839976185, 8...</td>\n",
       "      <td>[421519, 421510, 421504, 421510, 421509, 421511]</td>\n",
       "      <td>[119, 80, 63, 81, 79, 68, 118, 80, 62, 83, 87,...</td>\n",
       "      <td>[600, 33, 96, 22, 22, 49, 298, 23, 23, 128, 11...</td>\n",
       "      <td>[38, 24, 32, 30, 26, 32700]</td>\n",
       "      <td>82.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2606470</td>\n",
       "      <td>2606470</td>\n",
       "      <td>2964683</td>\n",
       "      <td>183</td>\n",
       "      <td>2964683</td>\n",
       "      <td>163</td>\n",
       "      <td>249</td>\n",
       "      <td>2</td>\n",
       "      <td>44900</td>\n",
       "      <td>58904</td>\n",
       "      <td>...</td>\n",
       "      <td>2606470.0</td>\n",
       "      <td>[20164962, 20165173, 20165063, 20165181, 20165...</td>\n",
       "      <td>[839802875, 839802890, 839802867, 839802804, 8...</td>\n",
       "      <td>[421987, 421985, 421986, 421988, 421982]</td>\n",
       "      <td>[36, 28, 44, 35, 23, 42, 39, 39, 38, 29, 35, 2...</td>\n",
       "      <td>[979, 481, 478, 412, 207]</td>\n",
       "      <td>[36, 34, 40, 30, 32700]</td>\n",
       "      <td>36.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 156 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_ciclo_acarreo  id_cargadescarga  id_palas  id_equipo_camion  \\\n",
       "0           2606466           2606466   2964682                41   \n",
       "1           2606467           2606467   2964686                43   \n",
       "2           2606468           2606468   2964690                44   \n",
       "3           2606469           2606469   2964684                52   \n",
       "4           2606470           2606470   2964683               183   \n",
       "\n",
       "   id_ciclo_carguio  id_equipo_carguio  id_trabajador_pala  \\\n",
       "0           2964682                 26                 221   \n",
       "1           2964686                 27                 232   \n",
       "2           2964690                 27                 232   \n",
       "3           2964684                 24                 246   \n",
       "4           2964683                163                 249   \n",
       "\n",
       "   id_guardia_realiza_carga_al_camion  id_locacion  \\\n",
       "0                                   2        44919   \n",
       "1                                   2        44941   \n",
       "2                                   2        44941   \n",
       "3                                   2        44939   \n",
       "4                                   2        44900   \n",
       "\n",
       "   id_poligono_se_obtiene_material  ... id_cargadescarga_pases  \\\n",
       "0                            59473  ...              2606466.0   \n",
       "1                            59547  ...              2606467.0   \n",
       "2                            59547  ...              2606468.0   \n",
       "3                            59557  ...              2606469.0   \n",
       "4                            58904  ...              2606470.0   \n",
       "\n",
       "                                       coord_x_pases  \\\n",
       "0  [20463338, 20463338, 20463334, 20463338, 20463...   \n",
       "1  [20444297, 20444553, 20444495, 20444418, 20444...   \n",
       "2  [20444531, 20444317, 20444366, 20444322, 20445...   \n",
       "3  [20074133, 20074149, 20074146, 20074163, 20074...   \n",
       "4  [20164962, 20165173, 20165063, 20165181, 20165...   \n",
       "\n",
       "                                       coord_y_pases  \\\n",
       "0  [839692963, 839692966, 839692968, 839692971, 8...   \n",
       "1  [839667711, 839667632, 839667540, 839667665, 8...   \n",
       "2  [839668575, 839668163, 839668082, 839668073, 8...   \n",
       "3  [839976228, 839976242, 839976263, 839976185, 8...   \n",
       "4  [839802875, 839802890, 839802867, 839802804, 8...   \n",
       "\n",
       "                                       coord_z_pases  \\\n",
       "0   [418527, 418521, 418523, 418516, 418517, 418524]   \n",
       "1  [418456, 418454, 418455, 418455, 418457, 41846...   \n",
       "2  [418459, 418455, 418457, 418457, 418456, 41846...   \n",
       "3   [421519, 421510, 421504, 421510, 421509, 421511]   \n",
       "4           [421987, 421985, 421986, 421988, 421982]   \n",
       "\n",
       "                                   angulo_giro_pases  \\\n",
       "0  [87, 81, 79, 71, 65, 92, 91, 85, 75, 65, 69, 9...   \n",
       "1  [39, 1, 0, 1, 1, 21, 16, 1, 4, 3, 19, 13, 25, ...   \n",
       "2  [46, 29, 20, 22, 33, 35, 40, 46, 55, 17, 21, 1...   \n",
       "3  [119, 80, 63, 81, 79, 68, 118, 80, 62, 83, 87,...   \n",
       "4  [36, 28, 44, 35, 23, 42, 39, 39, 38, 29, 35, 2...   \n",
       "\n",
       "                                      tonelaje_pases  \\\n",
       "0                          [760, 393, 356, 453, 343]   \n",
       "1                     [768, 340, 337, 340, 297, 295]   \n",
       "2                [643, 280, 324, 238, 297, 344, 229]   \n",
       "3  [600, 33, 96, 22, 22, 49, 298, 23, 23, 128, 11...   \n",
       "4                          [979, 481, 478, 412, 207]   \n",
       "\n",
       "             duracion_excavacion_pases angulo_giro_promedio_pases  \\\n",
       "0          [26, 42, 22, 30, 32, 32700]                       80.0   \n",
       "1  [30, 36, 38, 38, 10, 28, 10, 32700]                       14.0   \n",
       "2  [40, 34, 38, 26, 12, 42, 38, 32700]                       34.0   \n",
       "3          [38, 24, 32, 30, 26, 32700]                       82.0   \n",
       "4              [36, 34, 40, 30, 32700]                       36.0   \n",
       "\n",
       "   has_block_pases  rownum  \n",
       "0             True       1  \n",
       "1             True       1  \n",
       "2             True       1  \n",
       "3             True       1  \n",
       "4             True       1  \n",
       "\n",
       "[5 rows x 156 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from minio import Minio\n",
    "import urllib3\n",
    "\n",
    "# Conectar al servidor de MinIO\n",
    "client2 = Minio(\n",
    "    \"minio.minio-user.svc.cluster.local:80\",  # IP del servidor MinIO (mydomain.com)\n",
    "    access_key=\"PTDkCupJb66cl9DwvmlW\",\n",
    "    secret_key=\"AkVxpH7t4og1DK6VIJo8jLrdQTBz9a6ZhzOYYT6n\",\n",
    "    secure=False,  # Cambia a True si usas HTTPS\n",
    "    http_client=urllib3.PoolManager(cert_reqs='CERT_NONE')  # Ignora la verificación del certificado\n",
    ")\n",
    "\n",
    "# Nombre del bucket y del archivo\n",
    "bucket_name = 'hudbayraw'\n",
    "object_name = 'data_pases_2023-06-01_2023-07-01.parquet'  # Nombre del archivo Parquet que quieres leer\n",
    "\n",
    "# Obtener el archivo Parquet del bucket\n",
    "response = client2.get_object(bucket_name, object_name)\n",
    "data = response.read()\n",
    "\n",
    "# Convertir los datos leídos en un DataFrame de Pandas\n",
    "parquet_buffer = BytesIO(data)\n",
    "datos = pd.read_parquet(parquet_buffer)\n",
    "\n",
    "# Mostrar el DataFrame\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f8e881-b44c-46ed-851f-d2c2be90ca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Tratamiento de valores Nulos\n",
    "# 3.1 Supongamos que tienes un DataFrame llamado datos\n",
    "valores_nulos = datos.isnull().sum()\n",
    "valores_nulos_ordenados = valores_nulos.sort_values(ascending=False)\n",
    "porcentaje_nulos = (valores_nulos_ordenados / len(datos)) * 100\n",
    "\n",
    "# 3.2 Eliminamos las variables que tienen mas de 80% Nulos\n",
    "columnas_a_eliminar = porcentaje_nulos[porcentaje_nulos > 80].index\n",
    "datos = datos.drop(columnas_a_eliminar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf2cc7e-b712-4e20-b363-5e1ad0a08f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    4,    0,    0,    0,    0,    0,    0,    0,\n",
       "       3299,    0, 3686,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0, 9505,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,   86,    0,    0,    0,    0,    0,    0,   86,   97,\n",
       "          0,    0,  237,  109,    5,    5,    5,  140,    0,  143,    0,\n",
       "          0,    5,    5,   79,   84,  307,    7,   97,    0,    0,    5,\n",
       "        142,  143,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 5899,\n",
       "       5899,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    1,    1,    1,  481,  481,  481,  481,  481,  481,\n",
       "        481,  481,    0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2fdeccc-cb8e-4008-8d53-c1feffbdc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.4 Rellenar los valores nulos con ceros en todo el DataFrame\n",
    "datos = datos.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b8647e7-61f9-47e1-836b-16c1a20bb81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.isnull().sum().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24fb83a6-c13c-4523-afbd-84e5c320a571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [20463338, 20463338, 20463334, 20463338, 20463...\n",
       "1        [20444297, 20444553, 20444495, 20444418, 20444...\n",
       "2        [20444531, 20444317, 20444366, 20444322, 20445...\n",
       "3        [20074133, 20074149, 20074146, 20074163, 20074...\n",
       "4        [20164962, 20165173, 20165063, 20165181, 20165...\n",
       "                               ...                        \n",
       "27774    [20441859, 20441859, 20441855, 20441856, 20441...\n",
       "27775    [20170141, 20170973, 20170148, 20170990, 20170...\n",
       "27776    [20441848, 20441849, 20441849, 20441850, 20441...\n",
       "27777             [20459475, 20459467, 20459466, 20459467]\n",
       "27778    [20094231, 20094490, 20094011, 20094742, 20094...\n",
       "Name: coord_x_pases, Length: 27779, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos['coord_x_pases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5892f6b-6f2b-4ab8-8058-a4e3e9add1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['previous_esperando_pala', 'estado_detalle_camion',\n",
       "       'estado_secundario_camion', 'estado_primario_camion',\n",
       "       'estado_detalle_pala', 'estado_secundario_pala', 'estado_primario_pala',\n",
       "       'nombre_equipo', 'flota_secundaria', 'flota_principal',\n",
       "       'nombre_equipo_carguio', 'nombre_turnocarga', 'nombre_turnodescarga',\n",
       "       'nombre_descarga', 'nombre_carga_locacion', 'ids_poligonos_en_locacion',\n",
       "       'nombre_poligono', 'ley_in', 'coord_x_pases', 'coord_y_pases',\n",
       "       'coord_z_pases', 'angulo_giro_pases', 'tonelaje_pases',\n",
       "       'duracion_excavacion_pases', 'has_block_pases'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4a122766-b52e-4f0d-b865-f0933117d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos['estado_detalle_camion']=datos['estado_detalle_camion'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb1db44e-9f2e-45c0-b52b-5ad320eedd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id_ciclo_acarreo                int64\n",
      "id_cargadescarga                int64\n",
      "id_palas                        int64\n",
      "id_equipo_camion                int64\n",
      "id_ciclo_carguio                int64\n",
      "                               ...   \n",
      "tonelaje_pases                 object\n",
      "duracion_excavacion_pases      object\n",
      "angulo_giro_promedio_pases    float64\n",
      "has_block_pases                object\n",
      "rownum                          int64\n",
      "Length: 146, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Convertir todo las columnas de tipo OBJECT  a  tipo de datos String , para poder transformar de df pandas a df-spark\n",
    "import pandas as pd\n",
    "\n",
    "# Convertir todas las columnas de tipo object a string\n",
    "for col in datos.select_dtypes(include=['object']).columns:\n",
    "    datos[col] = datos[col].astype(str)\n",
    "\n",
    "# Verifica que los tipos de datos han cambiado\n",
    "print(datos.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d49b4ee3-26d6-47b2-b507-828fa65dcda2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'),\n",
       "       dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'),\n",
       "       dtype('int64'), dtype('int64'),\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)], dtype('float64'),\n",
       "       dtype('float64'), dtype('O'), dtype('bool'), dtype('bool'),\n",
       "       dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'),\n",
       "       datetime64[ns, pytz.FixedOffset(-300)], dtype('float64'),\n",
       "       dtype('int64'), dtype('float64'), dtype('int64'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('int64'), dtype('int64'),\n",
       "       dtype('int64'), datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       dtype('int64'), dtype('int64'), dtype('float64'), dtype('int64'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('int64'), dtype('int64'),\n",
       "       dtype('int64'), datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)],\n",
       "       datetime64[ns, pytz.FixedOffset(-300)], dtype('int64'),\n",
       "       dtype('int64'), dtype('float64'), dtype('float64'), dtype('bool'),\n",
       "       dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'),\n",
       "       dtype('int64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('int64'),\n",
       "       dtype('float64'), dtype('bool'), dtype('bool'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('bool'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('O'), dtype('int64'),\n",
       "       dtype('O'), dtype('int64'), dtype('O'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('bool'), dtype('int64'),\n",
       "       dtype('int64'), dtype('int64'), dtype('O'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('int64'), dtype('O'), dtype('int64'),\n",
       "       dtype('int64'), dtype('int64'), dtype('O'), dtype('int64'),\n",
       "       dtype('int64'), dtype('int64'), dtype('int64'), dtype('int64'),\n",
       "       dtype('int64'), dtype('O'), dtype('O'), dtype('int64'),\n",
       "       dtype('float64'), dtype('O'), dtype('int64'), dtype('O'),\n",
       "       dtype('int64'), dtype('O'), dtype('float64'), dtype('float64'),\n",
       "       dtype('float64'), dtype('float64'), dtype('float64'), dtype('O'),\n",
       "       dtype('O'), dtype('O'), dtype('O'), dtype('O'), dtype('O'),\n",
       "       dtype('float64'), dtype('O'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.dtypes.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b27797-06a1-4195-b1c7-6dea28b8e2b1",
   "metadata": {},
   "source": [
    "### Convertir el DataFrame a Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0308b3a3-6a2c-461b-89e9-617384e2a120",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `coord_x_pases`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1630\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1673\u001b[0m     )\n\u001b[1;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `ndarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1681\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1678\u001b[0m     fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m         StructField(\n\u001b[1;32m   1680\u001b[0m             k,\n\u001b[0;32m-> 1681\u001b[0m             \u001b[43m_infer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m                \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m                \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1687\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1688\u001b[0m         )\n\u001b[1;32m   1689\u001b[0m     )\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1636\u001b[0m, in \u001b[0;36m_infer_type\u001b[0;34m(obj, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1637\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_DATA_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1638\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   1639\u001b[0m     )\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [UNSUPPORTED_DATA_TYPE] Unsupported DataType `ndarray`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandasToDelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Convertir el DataFrame de pandas a DataFrame de Spark\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# # Guardar el DataFrame de Spark como una tabla Delta\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# delta_path = \"/tmp/delta_table\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# spark_df.write.format(\"delta\").save(delta_path)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1436\u001b[0m     data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data, columns\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\n\u001b[1;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(\n\u001b[1;32m   1444\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:363\u001b[0m, in \u001b[0;36mSparkConversionMixin.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    362\u001b[0m converted_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_from_pandas(data, schema, timezone)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:955\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m--> 955\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/session.py:958\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[1;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m--> 958\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/types.py:1691\u001b[0m, in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         fields\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1679\u001b[0m             StructField(\n\u001b[1;32m   1680\u001b[0m                 k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1688\u001b[0m             )\n\u001b[1;32m   1689\u001b[0m         )\n\u001b[1;32m   1690\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m-> 1691\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   1692\u001b[0m             error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_TYPE_FOR_FIELD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1693\u001b[0m             message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfield_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: k},\n\u001b[1;32m   1694\u001b[0m         )\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StructType(fields)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_TYPE_FOR_FIELD] Unable to infer the type of the field `coord_x_pases`."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "# Iniciar una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"PandasToDelta\").getOrCreate()\n",
    "\n",
    "# Convertir el DataFrame de pandas a DataFrame de Spark\n",
    "spark_df = spark.createDataFrame(datos)\n",
    "\n",
    "# # Guardar el DataFrame de Spark como una tabla Delta\n",
    "# delta_path = \"/tmp/delta_table\"\n",
    "# spark_df.write.format(\"delta\").save(delta_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
